library(party)
library(mboost)
library(DMwR)
library(ROSE)
library(caTools)
library(kernlab)
dat <- read_csv(file = "sesiones.csv")
summary(dat)
dat$userAgent <- factor(dat$userAgent)
summary(dat)
dat$userAgent <- factor(dat$userAgent)
pre <- preProcess(x = dat, method = c("center"))
pre
summary(predict(object = pre, newdata = dat))
pre <- preProcess(x = dat, method = c("center", "scale"))
pre
summary(predict(object = pre, newdata = dat))
pre <- preProcess(x = as.data.frame(dat), method = c("center", "scale", "BoxCox"))
pre <- preProcess(x = as.data.frame(dat), method = c("center", "scale", "BoxCox"))
pre
pre
boxDat <- predict(object = pre, newdata = as.data.frame(dat))
apply(X = dat[,1:7], MARGIN = 2, FUN = skewness)
apply(X = boxDat[,1:7], MARGIN = 2, FUN = skewness)
qplot(x = sessionTime, geom = "density", data = dat)
qplot(x = sessionTime, geom = "density", data = boxDat)
dat <- as.data.frame(dat)
pre <- preProcess(x = dat, method = "spatialSign")
pre
spaDat <- predict(object = pre, newdata = dat)
qplot(data = dat, x = clicks, y =categories)
qplot(data = dat, x = clicks, y =categories)
qplot(data = spaDat, x = clicks, y = categories)
pre <- preProcess(x = dat, method = "spatialSign")
pre
spaDat <- predict(object = pre, newdata = dat)
qplot(data = dat, x = clicks, y =categories)
qplot(data = spaDat, x = clicks, y = categories)
qplot(data = spaDat, x = clicks, y = categories)
pre <- preProcess(x = dat, method = c("BoxCox", "center", "scale"))
qplot(data = spaDat, x = clicks, y = categories)
pre <- preProcess(x = dat, method = c("BoxCox", "center", "scale"))
plot(prcomp(x = predict(object = pre,newdata = dat)[,-8]), type = "l")
plot(prcomp(x = predict(object = pre,newdata = dat)[,-8]), type = "l")
pre <- preProcess(x = dat, method = c("BoxCox", "pca"), pcaComp = 5)
pre
1+1
# Hello World Example Function
hello <- function( name ) {
sprintf( "Hello, %s", name );
}
# Call Hello World Function
hello("Caio")
# Installing Arules Packages
install.packages("ggplot2")
install.packages('rJava')
Sys.getenv("R_HOME")
Sys.getenv("R_LIBS_USER")
library(datasets)
library(ggplot2)
library(datasets)
iris
install.packages("rwunderground")
library(rwunderground)
require(rwunderground)
forecast10day(set_location(territory = "Hawaii", city = "Honolulu"))
madrid_airport_weather -> forecast10day(set_location(airport_code = "MAD"))
madrid_airport_weather <- forecast10day(set_location(airport_code = "MAD"))
madrid_airport_weather <- forecast10day(set_location(airport_code = "MAD"))
forecast10day(set_location(airport_code = "SEA"))
forecast10day(set_location(territory = "Hawaii", city = "Honolulu"))
forecast10day(set_location(airport_code = "SEA"))
forecast10day(set_location(zip_code = "90210"))
forecast10day(set_location(territory = "IR", city = "Tehran"))
forecast10day(set_location(territory = "Hawaii", city = "Honolulu"))
forecast10day(set_location(territory = "Madrid", city = "Madrid"))
forecast10day(set_location(country="Spain", territory = "Madrid", city = "Madrid"))
forecast10day
forecast10day?
?forecast10day
?rwunderground
list_airports()
list_airports(Country="Spain")
list_airports().Country="Spain"
list_airports()
set_api_key(key = "5a92b38687c5327f")
set_api_key(key = "5a92b38687c5327f")
is_valid_airport()
lookup_country_code(name = "Madrid")
lookup_country_code(name = "New York")
lookup_country_code(name = "JFK")
astronomy(set_location(airport_code = "SEA"))
astronomy(set_location(airport_code = "JFK"))
forecast10day(set_location(territory = "Hawaii", city = "Honolulu"))
forecast10day(set_location(airport_code = "JFK"))
forecast10day(set_location(country="Spain", territory = "Madrid", city = "Madrid"))
forecast10day(set_location(zip_code = "90210"))
forecast10day(set_location(territory = "IR", city = "Tehran"))
forecast10day(set_location(country="Spain", territory = "Madrid", city = "Madrid"))
astronomy(set_location(airport_code = "JFK"))
conditions(set_location(territory = "Hawaii", city = "Honolulu"))
forecast3day(set_location(territory = "Hawaii", city = "Honolulu"))
forecast3day(set_location(city = "Madrid"))
forecast3day(set_location(country = "Spain", city = "Madrid"))
list_countries()
forecast3day(set_location(country = "ES", city = "Madrid"))
forecast3day(set_location(country.name.en = "ES", city = "Madrid"))
forecast3day(set_location(iso2c = "ES", city = "Madrid"))
forecast3day(set_location(iso2c = "ES"))
forecast3day(set_location(territory = "ES"))
forecast3day(set_location(territory = "ES"))
forecast3day(set_location(territory = "ES", city = "Madrid"))
forecast3day(set_location(territory = "ES", city = "Madrid"))
forecast10day(set_location(territory = "ES", city = "Madrid"))
list_countries()
list_states()
list_airports()
geolookup(set_location(territory = "Hawaii", city = "Honolulu"))
geolookup(set_location(territory = "ES", city = "Madrid"))
history(set_location(territory = "ES", city = "Madrid")), "20160101")
history(set_location(territory = "Hawaii", city = "Honolulu"), "20130101")
history(set_location(territory = "ES", city = "Madrid"), "20160101")
history(set_location(territory = "ES", city = "Madrid"), "20170508")
list_countries()
history(set_location(territory = "UK", city = "London"), "20170508")
hourly(set_location(territory = "Hawaii", city = "Honolulu"))
lookup_airport("Honolulu")
lookup_airport("Madrid")
forecast10day(set_location(airport_code = "Barajas"))
forecast10day(set_location(airport_code = "MAD"))
planner(set_location(territory = "Hawaii", city = "Honolulu"),
start_date = "0101", end_date = "0131")
rawtide(set_location(territory = "Hawaii", city = "Honolulu"))
satellite(set_location(territory = "Hawaii", city = "Honolulu"))
set_location(zip_code = "90210")
set_location(territory = "Hawaii", city = "Honolulu")
set_location(territory = "Kenya", city = "Mombasa")
set_location(airport_code = "SEA")
set_location(PWS_id = "KMNCHASK10")
set_location(PWS_id = "KMNCHASK10")
set_location(lat_long="40.6892,-74.0445")
set_location(autoip = "172.227.205.140")
set_location()
set_location()
library(sparklyr)
library(dplyr)
library(tidyr)
library(titanic)
library(ggplot2)
library(purrr)
install.packages("titanic")
library(sparklyr)
library(dplyr)
library(tidyr)
library(titanic)
library(ggplot2)
library(purrr)
sc <- spark_connect(master = "local", version = "2.0.0")
sc <- spark_connect(master = "local", version = "2.0.0")
spark_read_parquet(sc, name = "titanic", path = "titanic-parquet")
spark_read_parquet(sc, name = "titanic", path = "titanic-parquet")
setwd("~/GitHub/caiomsouza/machine-learning-orchestration/samples/apache-spark-ml/titanic")
setwd("~/GitHub/caiomsouza/machine-learning-orchestration/samples/apache-spark-ml/titanic")
spark_read_parquet(sc, name = "titanic", path = "titanic-parquet")
titanic_tbl <- tbl(sc, "titanic")
titanic2_tbl <- titanic_tbl %>%
mutate(Family_Size = SibSp + Parch + 1L) %>%
mutate(Pclass = as.character(Pclass)) %>%
filter(!is.na(Embarked)) %>%
mutate(Age = if_else(is.na(Age), mean(Age), Age)) %>%
sdf_register("titanic2")
titanic_tbl <- tbl(sc, "titanic")
titanic_tbl
titanic2_tbl <- titanic_tbl %>%
mutate(Family_Size = SibSp + Parch + 1L) %>%
mutate(Pclass = as.character(Pclass)) %>%
filter(!is.na(Embarked)) %>%
mutate(Age = if_else(is.na(Age), mean(Age), Age)) %>%
sdf_register("titanic2")
titanic_final_tbl <- titanic2_tbl %>%
mutate(Family_Size = as.numeric(Family_size)) %>%
sdf_mutate(
Family_Sizes = ft_bucketizer(Family_Size, splits = c(1,2,5,12))
) %>%
mutate(Family_Sizes = as.character(as.integer(Family_Sizes))) %>%
sdf_register("titanic_final")
partition <- titanic_final_tbl %>%
mutate(Survived = as.numeric(Survived), SibSp = as.numeric(SibSp), Parch = as.numeric(Parch)) %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, Family_Sizes) %>%
sdf_partition(train = 0.75, test = 0.25, seed = 8585)
train_tbl <- partition$train
test_tbl <- partition$test
partition <- titanic_tbl %>%
mutate(Survived = as.numeric(Survived), SibSp = as.numeric(SibSp), Parch = as.numeric(Parch)) %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked, Family_Sizes) %>%
sdf_partition(train = 0.75, test = 0.25, seed = 8585)
titanic_tbl
partition <- titanic_tbl %>%
mutate(Survived = as.numeric(Survived), SibSp = as.numeric(SibSp), Parch = as.numeric(Parch)) %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
sdf_partition(train = 0.75, test = 0.25, seed = 8585)
train_tbl <- partition$train
test_tbl <- partition$test
ml_formula <- formula(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked)
(ml_log <- ml_logistic_regression(train_tbl, ml_formula))
ml_dt <- ml_decision_tree(train_tbl, ml_formula)
ml_rf <- ml_random_forest(train_tbl, ml_formula)
ml_gbt <- ml_gradient_boosted_trees(train_tbl, ml_formula)
ml_nb <- ml_naive_bayes(train_tbl, ml_formula)
ml_nn <- ml_multilayer_perceptron(train_tbl, ml_formula, layers = c(11,15,2))
ml_models <- list(
"Logistic" = ml_log,
"Decision Tree" = ml_dt,
"Random Forest" = ml_rf,
"Gradient Boosted Trees" = ml_gbt,
"Naive Bayes" = ml_nb,
"Neural Net" = ml_nn
)
ml_nn <- ml_multilayer_perceptron(train_tbl, ml_formula, layers = c(11,15,2))
ml_models <- list(
"Logistic" = ml_log,
"Decision Tree" = ml_dt,
"Random Forest" = ml_rf,
"Gradient Boosted Trees" = ml_gbt,
"Naive Bayes" = ml_nb,
#  "Neural Net" = ml_nn
)
ml_models <- list(
"Logistic" = ml_log,
"Decision Tree" = ml_dt,
"Random Forest" = ml_rf,
"Gradient Boosted Trees" = ml_gbt,
"Naive Bayes" = ml_nb
)
score_test_data <- function(model, data=test_tbl){
pred <- sdf_predict(model, data)
select(pred, Survived, prediction)
}
ml_score <- lapply(ml_models, score_test_data)
calculate_lift <- function(scored_data) {
scored_data %>%
mutate(bin = ntile(desc(prediction), 10)) %>%
group_by(bin) %>%
summarize(count = sum(Survived)) %>%
mutate(prop = count / sum(count)) %>%
arrange(bin) %>%
mutate(prop = cumsum(prop)) %>%
select(-count) %>%
collect() %>%
as.data.frame()
}
ml_gains <- data.frame(bin = 1:10, prop = seq(0, 1, len = 10), model = "Base")
for(i in names(ml_score)){
ml_gains <- ml_score[[i]] %>%
calculate_lift %>%
mutate(model = i) %>%
rbind(ml_gains, .)
}
ggplot(ml_gains, aes(x = bin, y = prop, colour = model)) +
geom_point() + geom_line() +
ggtitle("Lift Chart for Predicting Survival - Test Data Set") +
xlab("") + ylab("")
calc_accuracy <- function(data, cutpoint = 0.5){
data %>%
mutate(prediction = if_else(prediction > cutpoint, 1.0, 0.0)) %>%
ml_classification_eval("prediction", "Survived", "accuracy")
}
perf_metrics <- data.frame(
model = names(ml_score),
AUC = 100 * sapply(ml_score, ml_binary_classification_eval, "Survived", "prediction"),
Accuracy = 100 * sapply(ml_score, calc_accuracy),
row.names = NULL, stringsAsFactors = FALSE)
gather(perf_metrics, metric, value, AUC, Accuracy) %>%
ggplot(aes(reorder(model, value), value, fill = metric)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
xlab("") +
ylab("Percent") +
ggtitle("Performance Metrics")
feature_importance <- data.frame()
for(i in c("Decision Tree", "Random Forest", "Gradient Boosted Trees")){
feature_importance <- ml_tree_feature_importance(sc, ml_models[[i]]) %>%
mutate(Model = i) %>%
mutate(importance = as.numeric(levels(importance))[importance]) %>%
mutate(feature = as.character(feature)) %>%
rbind(feature_importance, .)
}
feature_importance %>%
ggplot(aes(reorder(feature, importance), importance, fill = Model)) +
facet_wrap(~Model) +
geom_bar(stat = "identity") +
coord_flip() +
xlab("") +
ggtitle("Feature Importance")
summary(ml_dt)
ml_dt
ml_dt$features
ml_dt$response
ml_dt
plot(ml_dt)
tree(ml_dt)
(ml_log <- ml_logistic_regression(train_tbl, ml_formula))
ml_dt <- ml_decision_tree(train_tbl, ml_formula)
(ml_dt <- ml_decision_tree(train_tbl, ml_formula))
(ml_rf <- ml_random_forest(train_tbl, ml_formula))
(ml_rf <- ml_random_forest(train_tbl, ml_formula, trees = 30))
(ml_rf <- ml_random_forest(train_tbl, ml_formula, trees = 4))
ml_rf <- ml_random_forest(train_tbl, ml_formula)
ml_rf
ml_rf$num.trees
ml_rf$data
ml_rf$feature.importances
ml_rf$features
ml_rf$feature.importances
ml_gbt <- ml_gradient_boosted_trees(train_tbl, ml_formula)
ml_gbt
ml_gbt$trees
plot(ml_gbt$trees)
ml_nb <- ml_naive_bayes(train_tbl, ml_formula)
ml_nb
ml_nn <- ml_multilayer_perceptron(train_tbl, ml_formula, layers = c(11,15,2))
ml_nn <- ml_multilayer_perceptron(train_tbl, ml_formula, layers = c(8,15,2))
ml_nn
ml_nn$features
ml_models <- list(
"Logistic" = ml_log,
"Decision Tree" = ml_dt,
"Random Forest" = ml_rf,
"Gradient Boosted Trees" = ml_gbt,
"Naive Bayes" = ml_nb,
"Neural Net" = ml_nn
)
score_test_data <- function(model, data=test_tbl){
pred <- sdf_predict(model, data)
select(pred, Survived, prediction)
}
ml_score <- lapply(ml_models, score_test_data)
ml_score
calculate_lift <- function(scored_data) {
scored_data %>%
mutate(bin = ntile(desc(prediction), 10)) %>%
group_by(bin) %>%
summarize(count = sum(Survived)) %>%
mutate(prop = count / sum(count)) %>%
arrange(bin) %>%
mutate(prop = cumsum(prop)) %>%
select(-count) %>%
collect() %>%
as.data.frame()
}
ml_gains <- data.frame(bin = 1:10, prop = seq(0, 1, len = 10), model = "Base")
for(i in names(ml_score)){
ml_gains <- ml_score[[i]] %>%
calculate_lift %>%
mutate(model = i) %>%
rbind(ml_gains, .)
}
ggplot(ml_gains, aes(x = bin, y = prop, colour = model)) +
geom_point() + geom_line() +
ggtitle("Lift Chart for Predicting Survival - Test Data Set") +
xlab("") + ylab("")
calc_accuracy <- function(data, cutpoint = 0.5){
data %>%
mutate(prediction = if_else(prediction > cutpoint, 1.0, 0.0)) %>%
ml_classification_eval("prediction", "Survived", "accuracy")
}
perf_metrics <- data.frame(
model = names(ml_score),
AUC = 100 * sapply(ml_score, ml_binary_classification_eval, "Survived", "prediction"),
Accuracy = 100 * sapply(ml_score, calc_accuracy),
row.names = NULL, stringsAsFactors = FALSE)
gather(perf_metrics, metric, value, AUC, Accuracy) %>%
ggplot(aes(reorder(model, value), value, fill = metric)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
xlab("") +
ylab("Percent") +
ggtitle("Performance Metrics")
feature_importance <- data.frame()
for(i in c("Decision Tree", "Random Forest", "Gradient Boosted Trees")){
feature_importance <- ml_tree_feature_importance(sc, ml_models[[i]]) %>%
mutate(Model = i) %>%
mutate(importance = as.numeric(levels(importance))[importance]) %>%
mutate(feature = as.character(feature)) %>%
rbind(feature_importance, .)
}
feature_importance %>%
ggplot(aes(reorder(feature, importance), importance, fill = Model)) +
facet_wrap(~Model) +
geom_bar(stat = "identity") +
coord_flip() +
xlab("") +
ggtitle("Feature Importance")
library(sparklyr)
library(dplyr)
library(tidyr)
library(titanic)
library(ggplot2)
library(purrr)
setwd("~/GitHub/caiomsouza/machine-learning-orchestration/samples/apache-spark-ml/titanic")
sc <- spark_connect(master = "local", version = "2.0.0")
spark_read_parquet(sc, name = "titanic", path = "titanic-parquet")
titanic_tbl <- tbl(sc, "titanic")
titanic_tbl
titanic_tbl
partition <- titanic_tbl %>%
mutate(Survived = as.numeric(Survived), SibSp = as.numeric(SibSp), Parch = as.numeric(Parch)) %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
sdf_partition(train = 0.75, test = 0.25, seed = 8585)
train_tbl <- partition$train
test_tbl <- partition$test
ml_formula <- formula(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked)
(ml_log <- ml_logistic_regression(train_tbl, ml_formula))
ml_log <- ml_logistic_regression(train_tbl, ml_formula)
ml_dt <- ml_decision_tree(train_tbl, ml_formula)
ml_rf <- ml_random_forest(train_tbl, ml_formula)
ml_gbt <- ml_gradient_boosted_trees(train_tbl, ml_formula)
ml_gbt <- ml_gradient_boosted_trees(train_tbl, ml_formula)
ml_nb <- ml_naive_bayes(train_tbl, ml_formula)
ml_nn <- ml_multilayer_perceptron(train_tbl, ml_formula, layers = c(8,15,2))
ml_models <- list(
"Logistic" = ml_log,
"Decision Tree" = ml_dt,
"Random Forest" = ml_rf,
"Gradient Boosted Trees" = ml_gbt,
"Naive Bayes" = ml_nb,
"Neural Net" = ml_nn
)
score_test_data <- function(model, data=test_tbl){
pred <- sdf_predict(model, data)
select(pred, Survived, prediction)
}
ml_score <- lapply(ml_models, score_test_data)
#install.packages("titanic")
library(sparklyr)
library(dplyr)
library(tidyr)
library(titanic)
library(ggplot2)
library(purrr)
setwd("~/GitHub/caiomsouza/machine-learning-orchestration/samples/apache-spark-ml/titanic")
# Connect to local spark cluster and load data
sc <- spark_connect(master = "local", version = "2.0.0")
spark_read_parquet(sc, name = "titanic", path = "titanic-parquet")
titanic_tbl <- tbl(sc, "titanic")
#titanic_tbl
# Partition the data
partition <- titanic_tbl %>%
mutate(Survived = as.numeric(Survived), SibSp = as.numeric(SibSp), Parch = as.numeric(Parch)) %>%
select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
sdf_partition(train = 0.75, test = 0.25, seed = 8585)
# Create table references
train_tbl <- partition$train
test_tbl <- partition$test
# Model survival as a function of several predictors
ml_formula <- formula(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked)
# Train a logistic regression model
ml_log <- ml_logistic_regression(train_tbl, ml_formula)
## Decision Tree
ml_dt <- ml_decision_tree(train_tbl, ml_formula)
## Random Forest
ml_rf <- ml_random_forest(train_tbl, ml_formula)
## Gradient Boosted Tree
ml_gbt <- ml_gradient_boosted_trees(train_tbl, ml_formula)
## Naive Bayes
ml_nb <- ml_naive_bayes(train_tbl, ml_formula)
## Neural Network
ml_nn <- ml_multilayer_perceptron(train_tbl, ml_formula, layers = c(8,15,2))
# Bundle the modelss into a single list object
ml_models <- list(
"Logistic" = ml_log,
"Decision Tree" = ml_dt,
"Random Forest" = ml_rf,
"Gradient Boosted Trees" = ml_gbt,
"Naive Bayes" = ml_nb,
"Neural Net" = ml_nn
)
# Create a function for scoring
score_test_data <- function(model, data=test_tbl){
pred <- sdf_predict(model, data)
select(pred, Survived, prediction)
}
# Score all the models
ml_score <- lapply(ml_models, score_test_data)
library(sparklyr)
library(dplyr)
library(tidyr)
library(titanic)
library(ggplot2)
library(purrr)
setwd("~/GitHub/caiomsouza/machine-learning-orchestration/samples/apache-spark-ml/titanic")
sc <- spark_connect(master = "local", version = "2.0.0")
spark_read_parquet(sc, name = "titanic", path = "titanic-parquet")
titanic_tbl <- tbl(sc, "titanic")
titanic_tbl
titanic_tbl.df <- as.dataframe(titanic_tbl.df)
titanic_tbl.df <- as.data.frame(titanic_tbl)
titanic_tbl.df
#install.packages("titanic")
library(sparklyr)
library(dplyr)
library(tidyr)
library(titanic)
library(ggplot2)
library(purrr)
setwd("~/GitHub/caiomsouza/machine-learning-orchestration/samples/apache-spark-ml/titanic")
# Connect to local spark cluster and load data
sc <- spark_connect(master = "local", version = "2.0.0")
spark_read_parquet(sc, name = "titanic", path = "titanic-parquet")
titanic_tbl <- tbl(sc, "titanic")
titanic_tbl.df <- as.data.frame(titanic_tbl)
titanic_tbl.df
